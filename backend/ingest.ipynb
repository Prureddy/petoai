{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# data_processing.ipynb (Jupyter Notebook)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_experimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SemanticChunker\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_genai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleGenerativeAIEmbeddings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "# data_processing.ipynb (Jupyter Notebook)\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "# pdf_file_path = \"C:/Users/pruth/OneDrive/Desktop/PetCare/Chatbot/data/data.pdf\"\n",
    "\n",
    "text_file_path = \"output.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF\n",
    "# reader = PdfReader(pdf_file_path)\n",
    "# all_text = \"\".join([page.extract_text() for page in reader.pages])\n",
    "\n",
    "# Save extracted text to file\n",
    "# with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "#     text_file\n",
    "\n",
    "# print(f\"Text extracted and saved to {text_file_path}\")\n",
    "\n",
    "# Read the extracted text\n",
    "with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    extracted_text = file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "GOOGLE_API_KEY = \"AIzaSyCjhtUjPxDAfJSZiVXVWrf6U8Vnjfmgiwg\"\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Apply semantic chunking\n",
    "text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\", \n",
    "                                breakpoint_threshold_amount=70, min_chunk_size=512)\n",
    "docs = text_splitter.create_documents([extracted_text])\n",
    "\n",
    "print(f\"Total Chunks: {len(docs)}\")\n",
    "\n",
    "# Print first 3 chunks\n",
    "for i, doc in enumerate(docs[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(doc.page_content[:] + \"...\")  # Print first 200 characters of each chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'banking_chunks' exists. Reusing it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 3\n",
      "Add of existing embedding ID: 4\n",
      "Insert of existing embedding ID: 4\n",
      "Add of existing embedding ID: 5\n",
      "Insert of existing embedding ID: 5\n",
      "Add of existing embedding ID: 6\n",
      "Insert of existing embedding ID: 6\n",
      "Add of existing embedding ID: 7\n",
      "Insert of existing embedding ID: 7\n",
      "Add of existing embedding ID: 8\n",
      "Insert of existing embedding ID: 8\n",
      "Add of existing embedding ID: 9\n",
      "Insert of existing embedding ID: 9\n",
      "Add of existing embedding ID: 10\n",
      "Insert of existing embedding ID: 10\n",
      "Add of existing embedding ID: 11\n",
      "Insert of existing embedding ID: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing and storage completed!\n"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Initialize embedding function\n",
    "# Define your Google API key for embeddings\n",
    "GOOGLE_API_KEY = \"AIzaSyCjhtUjPxDAfJSZiVXVWrf6U8Vnjfmgiwg\"\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "DB_PATH = \"chroma_rag_data\"\n",
    "DB_NAME = \"banking_chunks\"\n",
    "\n",
    "def create_or_load_chroma_db(documents, path: str, name: str):\n",
    "    chroma_client = PersistentClient(path=path)\n",
    "    \n",
    "    # List existing collections (only names are returned in v0.6.0)\n",
    "    existing_collection_names = [col for col in chroma_client.list_collections()]\n",
    "    \n",
    "    if name in existing_collection_names:\n",
    "        collection = chroma_client.get_collection(name=name)\n",
    "        print(f\"Collection '{name}' exists. Reusing it.\")\n",
    "    else:\n",
    "        collection = chroma_client.create_collection(name=name, embedding_function=embeddings)\n",
    "        print(f\"Created new collection '{name}'.\")\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        document_text = doc.page_content if hasattr(doc, 'page_content') else str(doc)\n",
    "        try:\n",
    "            collection.add(documents=[document_text], ids=[str(i)], metadatas=[{\"chunk_index\": i}])\n",
    "        except chromadb.errors.DuplicateIDError:\n",
    "            print(f\"Skipping duplicate document {i}\")\n",
    "    \n",
    "    return collection\n",
    "\n",
    "db = create_or_load_chroma_db(docs, DB_PATH, DB_NAME)\n",
    "print(\"Data processing and storage completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
